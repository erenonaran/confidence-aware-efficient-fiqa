{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bPW1XKp4So_w"},"outputs":[],"source":["from google.colab import drive, userdata\n","import shutil\n","import os\n","\n","drive.mount('/content/drive', force_remount=True)\n","source_folder_path = '/content/drive/MyDrive/VLM_for_FIQA'\n","destination_folder_path = '/content/VLM_for_FIQA'\n","\n","shutil.copytree(source_folder_path, destination_folder_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnl67X2AZOwR"},"outputs":[],"source":["!pip install mtcnn --quiet\n","!pip install insightface --quiet\n","!pip install onnxruntime --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2GDUvdeTgAy"},"outputs":[],"source":["import zipfile\n","from mtcnn import MTCNN\n","from mtcnn.utils.images import load_images_batch\n","import glob\n","from tqdm import tqdm\n","from insightface.utils import face_align\n","from pathlib import Path\n","import cv2\n","import numpy as np\n","import os\n","import csv\n","import shutil\n","import matplotlib.pyplot as plt\n","import multiprocessing as mp\n","import torch\n","import concurrent.futures"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lwcunhFTqGX"},"outputs":[],"source":["zip_path = '/content/VLM_for_FIQA/CelebA-HQ/celeba.zip'\n","extract_path = '/content/celeba'\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sWg3sqAicbxT"},"outputs":[],"source":["def display_alignment(image_dict, detection_list):\n","\n","    detection_list = [detection_list] if not isinstance(detection_list, list) else detection_list\n","\n","    assert len(image_dict) == len(detection_list), \"Number of images and detections should be the same.\"\n","\n","    cols = 2\n","    rows = len(image_dict)\n","    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n","\n","    if rows == 1:\n","        axes = np.array([axes])\n","    axes = axes.reshape(rows, cols)\n","\n","    for i, (path, image, detection) in enumerate(zip(image_dict.keys(), image_dict.values(), detection_list)):\n","        aligned_image = align(image, detection)\n","\n","        original_image = image.copy()\n","        x, y, w, h = detection['box']\n","        cv2.rectangle(original_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","        for key, (px, py) in detection['keypoints'].items():\n","            cv2.circle(original_image, (px, py), 3, (0, 0, 255), -1)\n","            cv2.putText(original_image, key, (px + 5, py - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1)\n","\n","        axes[i, 0].imshow(original_image)\n","        axes[i, 0].set_title(f\"{path} - Original Image\")\n","        axes[i, 1].imshow(aligned_image)\n","        axes[i, 1].set_title(f\"{path} - Aligned Image\")\n","\n","    plt.setp(axes, xticks=[], yticks=[], frame_on=False)\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIaco_OFp9mS"},"outputs":[],"source":["def align(image, detection, output_size=224):\n","    \"\"\"Align a face image based on keypoint detection.\"\"\"\n","    landmark_arr = np.array([\n","        detection['keypoints']['left_eye'],\n","        detection['keypoints']['right_eye'],\n","        detection['keypoints']['nose'],\n","        detection['keypoints']['mouth_left'],\n","        detection['keypoints']['mouth_right']\n","    ])\n","\n","    aligned_image = face_align.norm_crop(image, landmark_arr, image_size=output_size)\n","    return aligned_image\n","\n","def worker(path, image, detection):\n","    \"\"\"Worker function for multiprocessing alignment.\"\"\"\n","    return path, align(image, detection)\n","\n","def align_batch(image_dict, detection_list):\n","    \"\"\"Align a batch of images in parallel using multiprocessing.\"\"\"\n","    assert len(image_dict) == len(detection_list), \"Number of images and detections should be the same.\"\n","    tasks = list(zip(image_dict.keys(), image_dict.values(), detection_list))\n","\n","    with mp.Pool(processes=mp.cpu_count()) as pool:\n","        results = pool.starmap(worker, tasks)\n","\n","    return {path: aligned_image for path, aligned_image in results}\n","\n","def display_alignment(image_dict, detection_list):\n","    \"\"\"Display original and aligned images with bounding boxes and landmarks.\"\"\"\n","\n","    if not isinstance(detection_list, list):\n","        detection_list = [detection_list]\n","\n","    assert len(image_dict) == len(detection_list), \"Number of images and detections should be the same.\"\n","\n","    cols = 2\n","    rows = len(image_dict)\n","    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n","\n","    if rows == 1:\n","        axes = np.expand_dims(axes, axis=0)\n","\n","    for i, (path, image, detection) in enumerate(zip(image_dict.keys(), image_dict.values(), detection_list)):\n","        aligned_image = align(image, detection)\n","\n","        original_image = image.copy()\n","\n","        x, y, w, h = detection['box']\n","        cv2.rectangle(original_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","\n","        for key, (px, py) in detection['keypoints'].items():\n","            cv2.circle(original_image, (px, py), 3, (0, 0, 255), -1)\n","\n","        axes[i, 0].imshow(original_image)\n","        axes[i, 0].set_title(f\"{path} - Original Image\")\n","        axes[i, 1].imshow(aligned_image)\n","        axes[i, 1].set_title(f\"{path} - Aligned Image\")\n","\n","    for ax in axes.flatten():\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","        ax.set_frame_on(False)\n","\n","    plt.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zkLimpWUaszH"},"outputs":[],"source":["batch_size = 128\n","\n","image_directory = Path(\"/content/celeba/celeba_hq_256\")\n","if not Path(image_directory).exists():\n","    raise FileNotFoundError(f\"The directory '{image_directory}' does not exist. Please check the path.\")\n","image_paths = [str(path) for path in image_directory.glob(\"*.jpg\")]\n","if not image_paths:\n","    raise FileNotFoundError(f\"No .jpg image files found in the specified directory: '{image_directory}'.\")\n","\n","output_dir = \"/content/celeba_mtcnn_aligned\"\n","if os.path.exists(output_dir):\n","  shutil.rmtree(output_dir)\n","os.makedirs(output_dir)\n","\n","device = \"GPU:0\" if torch.cuda.is_available() else \"CPU:0\"\n","detector = MTCNN(device=device)\n","print(f\"Using device: {device}\")\n","\n","failed_num = 0\n","for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Processing Batches\", unit=\"batch\"):\n","    batch_image_paths = image_paths[i:i + batch_size]\n","    batch_detections = detector.detect_faces(batch_image_paths,  batch_stack_justification=\"center\")\n","\n","    assert len(batch_detections) == len(batch_image_paths), \"Number of detections does not match the number of images in the batch.\"\n","\n","    valid_images = {}\n","    valid_detections = []\n","\n","    for detection, image_path in zip(batch_detections, batch_image_paths):\n","        image = cv2.imread(image_path)\n","\n","        if detection is None or len(detection) == 0 or image is None:\n","            failed_num += 1\n","            continue\n","\n","        best_detection = max(detection, key=lambda x: x['confidence']) if len(detection) > 1 else detection[0]\n","\n","        valid_detections.append(best_detection)\n","        valid_images[os.path.basename(image_path)] = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    assert (\n","        len(valid_detections) == len(valid_images)\n","    ), \"Number of valid detections does not match the number of images.\"\n","\n","    aligned_images = align_batch(valid_images, valid_detections)\n","\n","    for filename, aligned_image in aligned_images.items():\n","        output_path = os.path.join(output_dir, filename)\n","        cv2.imwrite(output_path, cv2.cvtColor(aligned_image, cv2.COLOR_RGB2BGR))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3uEdBSPgXaV"},"outputs":[],"source":["folder = '/content/celeba_mtcnn_aligned'\n","zip_filename = '/content/celeba_mtcnn_aligned'\n","shutil.make_archive(zip_filename, 'zip', folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRaNXO4agwEx"},"outputs":[],"source":["destination_directory = \"/content/drive/MyDrive/VLM_for_FIQA/CelebA-HQ\"\n","shutil.copy('/content/celeba_mtcnn_aligned.zip', destination_directory)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyMVPX9XpTtlYdyVaPqKXYhI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}